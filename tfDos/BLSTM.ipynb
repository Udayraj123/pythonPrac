{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "(6133, 39900)\n",
      "(6133, 700, 57)\n",
      "(5600, 700, 57)\n",
      "(277, 700, 57)\n",
      "(256, 700, 57)\n",
      "('Train data residues shape : ', (5600, 700, 21))\n",
      "('Train data secondary structue : ', (5600, 700, 8))\n",
      "('Train data n and c terminals : ', (5600, 700, 2))\n",
      "('Train data relative and absolute solvability : ', (5600, 700, 2))\n",
      "('Train data sequence profile : ', (5600, 700, 22))\n",
      "Successfully created the model\n",
      "(0, 0.060680795)\n"
     ]
    }
   ],
   "source": [
    "# Trying to reproduce results from\n",
    "# paper :    arXiv : 1412.7828v2 [q-bio.QM] 4 Jan 2015\n",
    "# Experiment name : Protein secondary structure prediction using\n",
    "# LSTM networks.\n",
    "\n",
    "# Model :\n",
    "#  -- Standard stacked bidirectional LSTM with 3 layers.\n",
    "#  -- (300 or 500) LSTM units in each layer\n",
    "#  -- There is a FFN between h_rec and h with a skip connection. h_rec = ffn(h) + h\n",
    "#  -- FFN is a two layer ReLU network with 300 or 500 units,\n",
    "#  -- Introduce a FFN to combine output from forward and backward RNN\n",
    "#  -- Has a ReLU with 200 or 400 hidden units.\n",
    "#  -- The concatenation is regularized with 50% dropout.\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "data = np.load('./data/cullpdb+profile_6133.npy.gz')\n",
    "print(data.shape)\n",
    "data = np.reshape(data, [6133, 700, 57])\n",
    "print(data.shape)\n",
    "\n",
    "# print(data.info())\n",
    "train_data = data[:5600, :]\n",
    "cv_data = data[5600:5877, :]\n",
    "test_data = data[5877:6133, :]\n",
    "\n",
    "print(train_data.shape)\n",
    "print(cv_data.shape)\n",
    "print(test_data.shape)\n",
    "\n",
    "\"\"\"\n",
    "Source : arXiv:1403.1347v1  [q-bio.QM]  6 Mar 2014\n",
    ":Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction\n",
    "\n",
    "The resulting training data including both feature and la-\n",
    "bels has 57 channels (22 for PSSM, 22 for sequence, 2 for\n",
    "terminals,  8  for  secondary  structure  labels,  2  for  solvent\n",
    "accessibility  labels),  and  the  overall  channel  size  is  700.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Source : http://www.princeton.edu/~jzthree/datasets/ICML2014/dataset_readme.txt\n",
    "It is currently in numpy format as a (N protein x k features) matrix. You can reshape it to (N protein x 700 amino acids x 57 features) first.\n",
    "\n",
    "The 57 features are:\n",
    "\"[0,22): amino acid residues, with the order of 'A', 'C', 'E', 'D', 'G', 'F',\n",
    "'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq'\"\n",
    "\"[22,31): Secondary structure labels, with the sequence of 'L', 'B', 'E', 'G', 'I', 'H',\n",
    "'S', 'T','NoSeq'\"\n",
    "\"[31,33): N- and C- terminals;\"\n",
    "\"[33,35): relative and absolute solvent accessibility, used only for training.\n",
    "(absolute accessibility is thresholded at 15; relative accessibility is normalized by the largest accessibility\n",
    "value in a protein and thresholded at 0.15; original solvent accessibility is computed by DSSP)\"\n",
    "\"[35,57): sequence profile. Note the order of amino acid residues is ACDEFGHIKLMNPQRSTVWXY and\n",
    "it is different from the order for amino acid residues\"\n",
    "\n",
    "The last feature of both amino acid residues and secondary structure labels just mark end of the protein sequence.\n",
    "\"[22,31) and [33,35) are hidden during testing.\"\n",
    "\n",
    "\n",
    "\"The dataset division for the first \"\"cullpdb+profile_6133.npy.gz\"\" dataset is\"\n",
    "\"[0,5600) training\"\n",
    "\"[5605,5877) test \"\n",
    "\"[5877,6133) validation\"\n",
    "\"\"\"\n",
    "# Split the train data\n",
    "train_data_residues = train_data[:, :,  0:21]\n",
    "train_data_secstruc = train_data[:, :, 22:30]\n",
    "train_data_nctermin = train_data[:, :, 31:33]\n",
    "train_data_rlabsolv = train_data[:, :, 33:35]\n",
    "train_data_sequepro = train_data[:, :, 35:57]\n",
    "\n",
    "# Checking shapes\n",
    "print(\"Train data residues shape : \", train_data_residues.shape)\n",
    "print(\"Train data secondary structue : \",train_data_secstruc.shape)\n",
    "print(\"Train data n and c terminals : \", train_data_nctermin.shape)\n",
    "print(\"Train data relative and absolute solvability : \", train_data_rlabsolv.shape)\n",
    "print(\"Train data sequence profile : \", train_data_sequepro.shape)\n",
    "\n",
    "train_data_input = train_data[:, :, np.r_[0:21, 36:57]]\n",
    "train_data_otput = train_data[:, :, 23:31]\n",
    "test_data_input = test_data[:, :, np.r_[0:21, 36:57]]\n",
    "test_data_otput = test_data[:, :, 23:31]\n",
    "# Checking shapes\n",
    "# print(\"Train data input  shape : \", train_data_input.shape)\n",
    "# print(\"Train data output shape : \", train_data_otput.shape)\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "num_classes = 8\n",
    "hidden_units = 30\n",
    "\n",
    "class BrnnForPssp():\n",
    "\n",
    "    def __init__(self, learning_rate, num_classes, hidden_units):\n",
    "\n",
    "        # Initialize data and variables\n",
    "        self.weights = tf.Variable(tf.random_uniform([hidden_units*2, num_classes], minval=-0.5, maxval=0.5))\n",
    "        self.biases  = tf.Variable(tf.random_uniform([num_classes]))\n",
    "        self.x = tf.placeholder(\"float\", [None, 700, 42])\n",
    "        self.y = tf.placeholder(\"float\", [None, 700, 8])\n",
    "\n",
    "        # Do the prediction\n",
    "        self.fw_rnn_cell1 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.fw_rnn_cell2 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.fw_rnn_cell3 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.bw_rnn_cell1 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.bw_rnn_cell2 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.bw_rnn_cell3 = rnn.LSTMCell(hidden_units, forget_bias=1.0, activation=tf.nn.relu)\n",
    "        self.fw_rnn_cells = [self.fw_rnn_cell1, self.fw_rnn_cell2, self.fw_rnn_cell3]\n",
    "        self.bw_rnn_cells = [self.bw_rnn_cell1, self.bw_rnn_cell2, self.bw_rnn_cell3]\n",
    "        self.outputs, self.states_fw, self.states_bw = rnn.stack_bidirectional_dynamic_rnn(\n",
    "                                                            self.fw_rnn_cells,\n",
    "                                                            self.bw_rnn_cells,\n",
    "                                                            self.x,\n",
    "                                                            dtype=tf.float32)\n",
    "        # self.output.shape is (?, 700, 600)\n",
    "        self.outputs_reshaped = tf.reshape(self.outputs, [-1, 2*hidden_units])\n",
    "        self.y_reshaped = tf.reshape(self.y, [-1, num_classes])\n",
    "        # check importantFunctions.py : line-40 to see how it works\n",
    "        # reference link  is :\n",
    "        # https://stackoverflow.com/questions/38051143/no-broadcasting-for-tf-matmul-in-tensorflow\n",
    "        self.y_predicted = tf.nn.softmax(tf.matmul(self.outputs_reshaped, self.weights) + self.biases)\n",
    "\n",
    "        # Define the loss function\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.y_predicted, labels=self.y_reshaped)\n",
    "\n",
    "        # Define the trainer and optimizer\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "        self.trainer = self.optimizer.minimize(self.loss)\n",
    "\n",
    "        # creating session and initializing variables\n",
    "        self.sess = tf.Session()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess.run(self.init)\n",
    "\n",
    "        # get accuracy\n",
    "        self.get_equal = tf.equal(tf.argmax(self.y_reshaped, 1), tf.argmax(self.y_predicted, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.get_equal, tf.float32))\n",
    "\n",
    "    def predict(self, x, y):\n",
    "        result = self.sess.run(self.y_predicted, feed_dict={self.x : x, self.y : y})\n",
    "        return result\n",
    "\n",
    "    def optimize(self, x, y):\n",
    "        result = self.sess.run(self.trainer, feed_dict={self.x : x, self.y : y})\n",
    "\n",
    "    def cross_validate(self, x, y):\n",
    "        result = self.sess.run(self.accuracy, feed_dict={self.x : x, self.y : y})\n",
    "        return result\n",
    "\n",
    "    def build_graph(self, x, y):\n",
    "        writer = tf.summary.FileWriter('./graphs/lstmForPSSP',self.sess.graph)\n",
    "\n",
    "\n",
    "\n",
    "model = BrnnForPssp(learning_rate=learning_rate, num_classes=8, hidden_units=hidden_units)\n",
    "print(\"Successfully created the model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0.060680792)\n",
      "(1, 0.060680792)\n",
      "(2, 0.060680784)\n",
      "(3, 0.060680792)\n",
      "(4, 0.060680788)\n",
      "(5, 0.060680795)\n",
      "(6, 0.060680792)\n",
      "(7, 0.060680792)\n",
      "(8, 0.060680795)\n",
      "(9, 0.060680792)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    j = i%57\n",
    "    x = train_data_input[j*100 : j*100+100, :]\n",
    "    y = train_data_otput[j*100 : j*100+100, :]\n",
    "    model.optimize(x=x, y=y)\n",
    "    if i % 1 == 0:\n",
    "        x = test_data_input\n",
    "        y = test_data_otput\n",
    "        print(i, model.cross_validate(x=x, y=y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
