{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import pickle\n",
    "import tempfile\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from nltk.util import ngrams\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration -\n",
    "num_ex = 3000#0\n",
    "prev_num = 3000\n",
    "newfile= prev_num != num_ex\n",
    "famTHR = 200*num_ex/500000\n",
    "seqlenTHR = 1001 #3400\n",
    "numgrams= 3 # for Glove\n",
    "# min_after_dequeue = 10000*num_ex/500000\n",
    "num_epochs=10\n",
    "batch_size = 100#0\n",
    "# dummy = \"end end end end end\"\n",
    "runonce=0\n",
    "alphabet =list(string.ascii_uppercase)\n",
    "alphabet = ['0']+alphabet\n",
    "vocab_size = len(alphabet)\n",
    "vector_size = 100\n",
    "filter_file='data/filtered_seqs_all.csv'\n",
    "corpus_file='data/filtered_seq_corpus.txt'\n",
    "glove_file='data/glove/vectors_pfam.txt'\n",
    "logs_path = '/tmp/tensorflow/logs'\n",
    "num_classes_file='uniprot_num_classes'\n",
    "\n",
    "# filepath='processed_uniprot.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printbuf(x):\n",
    "    sys.stdout.write(x)\n",
    "    sys.stdout.write('\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file data/uniprot_num_classes.pkl\n",
      "Loaded filtered_seqs\n",
      "('time taken: ', 10.306251049041748)\n"
     ]
    }
   ],
   "source": [
    "def save_obj(obj, name ,overwrite=1):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\tif(overwrite!=1 and os.path.exists(name)):\n",
    "\t\treturn [];\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print('File saved to '+filename)\n",
    "\n",
    "def load_obj(name):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\t# if(not os.path.exists(name)):\n",
    "\t# \treturn [];\n",
    "\twith open('data/' + name + '.pkl', 'rb') as f:\n",
    "\t\tprint('Loaded file '+filename)\n",
    "\t\treturn pickle.load(f)\n",
    "\n",
    "names = ['tri']\n",
    "for i in range(100):\n",
    "    names+= ['dim_'+str(i)]\n",
    "Glove = pd.read_table(glove_file,sep = ' ',header=None,index_col='tri',names=names)\n",
    "def genGlove(seq,n=3):\n",
    "    grams = ngrams(list(seq),n)\n",
    "#     grams = ngrams(eval(seq),n)\n",
    "    return map(lambda tri:list(Glove.loc[tri]),[''.join(g) for g in grams])\n",
    "#     return map(lambda tri:list(Glove.loc[tri]),[''.join(map(lambda x :alphabet[x],g)) for g in grams])\n",
    "\n",
    "def saveFilteredData():\n",
    "    fam='Cross-reference (Pfam)'\n",
    "    seq = 'Sequence'\n",
    "    main =pd.read_table(\"./data/uniprot-all.tab.gz\", sep='\\t',nrows=num_ex)\n",
    "    main[fam]= main[fam].apply(lambda x:str(x).split(';')[0])\n",
    "    col = (main[fam]!='nan') & (main[fam].isnull()==False) & (main[seq].isnull()==False)\n",
    "    main = main[col]\n",
    "#     length threshold\n",
    "    main = main[main['Length'].astype(int) < seqlenTHR]\n",
    "    \n",
    "    alphafam = list(main[fam].unique())\n",
    "    alphafam = [-1]+alphafam\n",
    "    num_classes = len(alphafam)\n",
    "    seq_records = main[[seq,fam,'Length']]\n",
    "    seq_records.columns=['seq','fam','length']\n",
    "    #All the heavy jobs here - takes upto 92 seconds\n",
    "#     seq_records['seq']= seq_records['seq'].apply(lambda seq : list(map(lambda x : alphabet.index(x),list(seq))))\n",
    "    \n",
    "    seq_records['fam']= seq_records['fam'].apply(lambda x : alphafam.index(x))\n",
    "    counts = seq_records.groupby(['fam']).size().reset_index(name='counts')\n",
    "    #10801 -> 9254 families\n",
    "    counts = counts[ (counts['counts']>=famTHR) & (counts['counts'] < 3400) ]\n",
    "    #608 -> 555 families having count between 200 and 3400\n",
    "    filtered_fams = list(counts['fam'])\n",
    "    #323162 -> 302907 examples total satisfy it.\n",
    "    filtered_seqs = seq_records[ seq_records['fam'].isin(filtered_fams)]\n",
    "    filtered_seqs = filtered_seqs.sort_values('length')\n",
    "    \n",
    "    filtered_seqs['seq'] = filtered_seqs['seq'].apply(lambda x:genGlove(x))\n",
    "#     Save for future use\n",
    "    save_obj(num_classes,num_classes_file)\n",
    "    filtered_seqs.to_csv(filter_file,index=False)\n",
    "    print('Filtered Seqs Saved to : '+filter_file)\n",
    "\n",
    "    # # takes 3 seconds !\n",
    "    # gb = seq_records.groupby('fam')    \n",
    "    # family_wise_db = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "if newfile or not os.path.exists(filter_file):\n",
    "    newfile=False\n",
    "    print('newFile or File not found, preparing data again')\n",
    "    p = time()\n",
    "    saveFilteredData()\n",
    "    print('time taken: ',time() - p)\n",
    "\n",
    "num_classes=load_obj(num_classes_file)\n",
    "p = time()\n",
    "filtered_seqs = pd.read_csv(filter_file,nrows=num_ex)\n",
    "print(\"Loaded filtered_seqs\")\n",
    "print('time taken: ',time() - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1486.000000</td>\n",
       "      <td>1486.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>106.561911</td>\n",
       "      <td>215.010094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>59.769477</td>\n",
       "      <td>101.906628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>64.500000</td>\n",
       "      <td>122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>105.000000</td>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>145.000000</td>\n",
       "      <td>315.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>274.000000</td>\n",
       "      <td>375.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fam       length\n",
       "count  1486.000000  1486.000000\n",
       "mean    106.561911   215.010094\n",
       "std      59.769477   101.906628\n",
       "min       1.000000    32.000000\n",
       "25%      64.500000   122.000000\n",
       "50%     105.000000   205.000000\n",
       "75%     145.000000   315.000000\n",
       "max     274.000000   375.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_seqs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pad(x,max_len):\n",
    "    xcp=eval(x) if(type(x)==str) else x #For further epochs\n",
    "    try :\n",
    "        return np.lib.pad(xcp,pad_width=[(0,max_len - len(xcp)),(0,0)],mode='constant',constant_values=(-1,0))    \n",
    "    except:\n",
    "        print(x)\n",
    "        print(\"Oops!\",sys.exc_info()[0],\"occured.\")\n",
    "        return np.lib.pad(xcp,pad_width=[(0,max_len - len(xcp)),(0,0)],mode='constant',constant_values=(-1,0))    \n",
    "#     return np.lib.pad(x,(0,max_len - len(x)),'constant',constant_values=(-1,0))\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.batch_no = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.batch_no=0\n",
    "            self.shuffle()#resets the cursor as well.\n",
    "        \n",
    "        res = self.df.iloc[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        self.batch_no+=1\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max(res['length'])\n",
    "        res2 = res.copy()\n",
    "        res2['seq'] = map(lambda x:pad(x,maxlen),res2['seq'])\n",
    "        return res2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RnnForPfcModelOne:\n",
    "#   @profile\n",
    "    def __init__(self, \n",
    "        num_classes = 549, \n",
    "        hidden_units=100,\n",
    "        learning_rate=0.01,\n",
    "         logs_path='/tmp/tensorflow/logs'\n",
    "                ):\n",
    "        global vocab_size\n",
    "        global vector_size\n",
    "        # batch_size * no_of_time_steps * vocab_size _/\n",
    "        self.weights = tf.Variable(tf.random_uniform(shape=[hidden_units, num_classes], maxval=1),name=\"Weights\")\n",
    "        self.biases = tf.Variable(tf.random_uniform(shape=[num_classes]),name=\"Biases\")\n",
    "        self.rnn_fcell = rnn.BasicLSTMCell(num_units = hidden_units, \n",
    "                                           forget_bias = 1.0,\n",
    "                                           activation = tf.tanh)\n",
    "        \"\"\"\n",
    "        We now have the glove file, containing vector spaces for all of the trigrams we have in vocab. So now rather than passing \n",
    "        the one-hot vectors, we'll pass the GloVes having reduced dimensions.\n",
    "        \"\"\"\n",
    "\n",
    "        # self.len_data taken from feed_dict\n",
    "        self.len_data = tf.placeholder(tf.uint8, [None],name=\"Lengths\")\n",
    "        # self.x_input taken from feed_dict - now it'll be a GloVe\n",
    "        self.x_input_g = tf.placeholder(tf.float32, [None, None,vector_size], name = 'X_GloVe') # batch_size * no_of_time_steps * vector_size\n",
    "#         self.x_input = tf.placeholder(tf.uint8, [None, None], name = 'x_ip') # batch_size * no_of_time_steps _/\n",
    "#         # self.x_input_o takes self.x_input\n",
    "#         self.x_input_o = tf.one_hot(indices = self.x_input, \n",
    "#             depth = vocab_size,\n",
    "#             on_value = 1.0,\n",
    "#             off_value = 0.0,\n",
    "#             axis = -1)\n",
    "        # self.outputs takes self.x_input_o & len_data\n",
    "        with tf.name_scope('DynamicRNN'):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(self.rnn_fcell,\n",
    "                                                      self.x_input_g,\n",
    "#                                                       self.x_input_o,\n",
    "                                                      sequence_length = self.len_data,\n",
    "                                                      dtype = tf.float32)\n",
    "        \n",
    "        # outputs of shape batch_size * no_of_time_steps * vocab_size\n",
    "        # output at time t i.e. the last output\n",
    "        self.outputs_t = tf.reshape(self.outputs[:, -1, :], [-1, hidden_units])\n",
    "        # The single layer NN to classify - takes outputs_t\n",
    "        self.y_predicted = tf.matmul(self.outputs_t, self.weights) + self.biases\n",
    "        \n",
    "        \n",
    "        # self.y_input taken from feed_dict batch_size *1\n",
    "        self.y_input = tf.placeholder(tf.uint8, [None], name = 'y_ip')\n",
    "        # self.y_input_o takes y_input\n",
    "        self.y_input_o = tf.one_hot(indices = self.y_input, \n",
    "                                    depth = num_classes,\n",
    "                                    on_value = 1.0,\n",
    "                                    off_value = 0.0,\n",
    "                                    axis = -1)\n",
    "        \n",
    "        # self.loss takes one hot y and y_predicted\n",
    "        with tf.name_scope('CrossEntropy'):\n",
    "            self.loss = tf.reduce_mean(\n",
    "                tf.nn.softmax_cross_entropy_with_logits(logits=self.y_predicted, labels=self.y_input_o)\n",
    "            )\n",
    "        #y_predicted and y_input_o shud be of same size = batch_size * num_classes\n",
    "        # define optimizer and trainer\n",
    "        with tf.name_scope('AdamOptimizer'):\n",
    "            self.trainer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n",
    "#             self.trainer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.get_equal = tf.equal(tf.argmax(self.y_input_o, 1), tf.argmax(self.y_predicted, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.get_equal, tf.float32),name=\"Accuracy\")\n",
    "        \n",
    "        self.summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        # Merge all summaries into a single op - to pass into sess.run\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#   @profile\n",
    "    def predict(self, x, y, len_data):\n",
    "        result = self.sess.run(self.y_predicted, feed_dict={self.x_input_g: x, self.y_input: y, self.len_data:len_data})\n",
    "        #result = self.sess.run(self.y_predicted, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def optimize(self, x, y, len_data,summary_index):\n",
    "#         c, summary = self.sess.run([self.loss, self.merged_summary_op],\n",
    "        c = self.sess.run([self.loss],\n",
    "                              feed_dict={self.x_input_g: x, self.y_input: y, self.len_data:len_data})\n",
    "                              #feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        \n",
    "        \n",
    "#         self.summary_writer.add_summary(summary, summary_index)\n",
    "\n",
    "        self.sess.run(self.trainer, feed_dict={self.x_input_g: x, self.y_input: y, self.len_data:len_data})\n",
    "        #self.sess.run(self.trainer, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        return c\n",
    "\n",
    "#   @profile\n",
    "    def cross_validate(self, x, y, len_data):\n",
    "        result = self.sess.run(self.accuracy, feed_dict={self.x_input_g:x, self.y_input:y, self.len_data:len_data})\n",
    "        #result = self.sess.run(self.accuracy, feed_dict={self.x_input:x, self.y_input:y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def close_summary_writer(self):\n",
    "        self.summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 1, 'Cost :', [5.0616164], 'Training acc : 0')\n",
      "(0, 2, 'Cost :', [5.1055784], 'Training acc : 0')\n",
      "(0, 3, 'Cost :', [5.0535107], 'Training acc : 0')\n",
      "(0, 4, 'Cost :', [5.0147557], 'Training acc : 0')\n",
      "(0, 5, 'Cost :', [4.9840946], 'Training acc : 0')\n",
      "(0, 6, 'Cost :', [5.0584993], 'Training acc : 0')\n",
      "(0, 7, 'Cost :', [5.0251369], 'Training acc : 0')\n",
      "(0, 8, 'Cost :', [5.0146003], 'Training acc : 0')\n",
      "(0, 9, 'Cost :', [4.9490094], 'Training acc : 0')\n",
      "(0, 10, 'Cost :', [4.9155455], 'Training acc : 0')\n",
      "(0, 11, 'Cost :', [4.9769063], 'Training acc : 0')\n",
      "(0, 12, 'Cost :', [4.9440641], 'Training acc : 0')\n",
      "(0, 13, 'Cost :', [4.8957925], 'Training acc : 0')\n",
      "(0, 14, 'Cost :', [4.911448], 'Training acc : 0')\n",
      "[[ 0.478664  0.194855  0.268756 ..., -0.001515 -0.385026 -0.380525]\n",
      " [-0.266599 -0.016745  0.140885 ..., -0.044535  0.383963 -0.04804 ]\n",
      " [-0.112923 -0.150274  0.282372 ...,  0.278006 -0.252773  0.280834]\n",
      " ..., \n",
      " [ 0.        0.        0.       ...,  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...,  0.        0.        0.      ]\n",
      " [ 0.        0.        0.       ...,  0.        0.        0.      ]]\n",
      "('Oops!', <type 'exceptions.ValueError'>, 'occured.')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[(0, -2), (0, 0)] cannot contain negative values.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-74b00b3a26a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPaddedDataIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     batch_x of shape batch_size * max_len * vector_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3089e4a676>\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Pad sequences with 0s so they are all the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3089e4a676>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Pad sequences with 0s so they are all the same length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mmaxlen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-0d3089e4a676>\u001b[0m in \u001b[0;36mpad\u001b[0;34m(x, max_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Oops!\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"occured.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpad_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_len\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxcp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'constant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconstant_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m#     return np.lib.pad(x,(0,max_len - len(x)),'constant',constant_values=(-1,0))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleDataIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36mpad\u001b[0;34m(array, pad_width, mode, **kwargs)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0mnarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m     \u001b[0mpad_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_width\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m     allowedkwargs = {\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/arraypad.pyc\u001b[0m in \u001b[0;36m_validate_lengths\u001b[0;34m(narray, number_elements)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m             \u001b[0mfmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s cannot contain negative values.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfmt\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnumber_elements\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1087\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnormshp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [(0, -2), (0, 0)] cannot contain negative values."
     ]
    }
   ],
   "source": [
    "if(not runonce):\n",
    "    runonce=1\n",
    "    model = RnnForPfcModelOne(num_classes=num_classes)\n",
    "    \n",
    "it = PaddedDataIterator(filtered_seqs)    \n",
    "while(it.epochs != num_epochs):\n",
    "    df = it.next_batch(batch_size)\n",
    "    batch_x, batch_y, len_data = map(list,(df['seq'],df['fam'],df['length']))\n",
    "#     batch_x of shape batch_size * max_len * vector_size\n",
    "    \n",
    "    cost = model.optimize(batch_x, batch_y, len_data,it.batch_no + batch_size*it.epochs)\n",
    "    accuracy_known = model.cross_validate(batch_x,batch_y,len_data)\n",
    "    print(it.epochs, it.batch_no,'Cost :',cost,\"Training acc : %d\" % (accuracy_known) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A function that generates words of 3 grams from the given list of sequences\n",
    "# Then returns without disturbing their 'context'\n",
    "def func(x):\n",
    "    if(x <0 or x>len(alphabet)):\n",
    "        print(\"IndeX\",x)\n",
    "    print(x)\n",
    "    return alphabet[x]\n",
    "\n",
    "def genCorpus(filtered_seqs,n=3):\n",
    "#     seq is already a string(list of integers) -\n",
    "    total = filtered_seqs['seq'].size    \n",
    "    print('generating corpus : ')\n",
    "    with open(corpus_file,'a') as f:\n",
    "        i =0\n",
    "        for rec in filtered_seqs.iterrows():\n",
    "            i+=1\n",
    "            grams = ngrams(list(rec[1].seq),n)\n",
    "            f.write(' '.join([''.join(g) for g in grams]))\n",
    "#             grams = ngrams(eval(rec[1].seq),n)\n",
    "#             f.write(' '.join([''.join(map(lambda x :alphabet[x],g)) for g in grams]))\n",
    "            f.write(' ')\n",
    "            perc = (i*100)/total\n",
    "            printbuf('%d %% completed' % perc)\n",
    "    print(\"Wrote corpus to file\")\n",
    "\n",
    "# if os.path.exists(corpus_file):\n",
    "#     with open(corpus_file,'r') as f:\n",
    "#         corpus = f.read()\n",
    "#         print('loaded corpus')\n",
    "# else :\n",
    "#     p = time()\n",
    "#     with open(corpus_file,'w') as f:\n",
    "#         f.write('')\n",
    "#     genCorpus(filtered_seqs,numgrams)\n",
    "#     print('time taken: ',time() - p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
