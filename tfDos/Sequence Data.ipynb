{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration -\n",
    "num_ex = 5000000\n",
    "famTHR = num_ex*200/500000\n",
    "num_epochs=1\n",
    "#pandas read - ,nrows=num_ex really speeds it up !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import tempfile\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def save_obj(obj, name ,overwrite=1):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\tif(overwrite==1 and os.path.exists(name)):\n",
    "\t\treturn [];\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\t# if(not os.path.exists(name)):\n",
    "\t# \treturn [];\n",
    "\twith open('data/' + name + '.pkl', 'rb') as f:\n",
    "\t\treturn pickle.load(f)\n",
    "\n",
    "fam='Cross-reference (Pfam)'\n",
    "main =pd.read_table(\"./data/uniprot-all.tab.gz\", sep='\\t',nrows=num_ex)\n",
    "main[fam]= main[fam].apply(lambda x:str(x).split(';')[0])\n",
    "alphafam = list(main[fam].unique())\n",
    "alphabet =list(string.ascii_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_records = main[['Sequence',fam]]\n",
    "seq_records.columns=['seq','fam']\n",
    "# seq_records.dropna() would do the same?!\n",
    "# col = (seq_records['fam'].isnull()==False) & (seq_records['seq'].isnull()==False)\n",
    "# seq_records = seq_records[col]\n",
    "seq_records = seq_records.dropna(how='any')\n",
    "sample = seq_records#.sample(num_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All the heavy jobs here -\n",
    "seq_records = sample.copy()\n",
    "seq_records['seq']= seq_records['seq'].apply(lambda seq : list(map(lambda x : 1+alphabet.index(x),list(seq))))\n",
    "seq_records['fam']= seq_records['fam'].apply(lambda x : 1+alphafam.index(x))\n",
    "#added 1+ for skipping 0 index\n",
    "counts = seq_records.groupby(['fam']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#10801 families.\n",
    "# counts = counts[ (counts['counts']>=famTHR) & (counts['counts'] < 3400) ]\n",
    "counts = counts[counts['counts'] > 3300 ]\n",
    "#608 families having count between 200 and 3400\n",
    "filtered_fams = list(counts['fam'])\n",
    "#323162 examples total satisfy it.\n",
    "seq_records = seq_records[ seq_records['fam'].isin(filtered_fams)]\n",
    "seq_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPadding = \\n    static padding = using FIFOQueue\\n    dynamic padding = using tf.train.next_batch(..,dynamic_pad=True)\\n    Bucketting = tf.contrib.training.bucket_by_sequence_length(..,dynamic_pad=True)\\n    \\nGoal : Handle sequences of unknown lengths\\ntf.while_loop = dynamic loops and supports backprop grad descent\\ntf.while_loop(cond_fn,body_fn,loop_vars,)\\n\\nBut now to be able to process Slices of Tensors, use tf.TensorArray()\\nnum_rows = matrix.shape[0]\\nta = tf.TensorArray(tf.float32,size=num_rows)\\nloadedMatrix = ta.unstack(matrix)\\nread_row = loadedMatrix.read(row_idx)\\nloadedMatrix = loadedMatrix.write(row_idx,fn(read_row))\\nmatrix = loadedMatrix.stack()\\n\\n'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x = pd.DataFrame([[1],[2],[1],[3]])\n",
    "x = x[(x==1) | (x==2)]\n",
    "x.dropna()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SequenceData can be stored into TFRecords that is tf's own storage. \n",
    "It is easier to handle and reusable\n",
    "& it follows the protocol buffer format already\n",
    "\"\"\"\n",
    "# efficient storage of the sequences\n",
    "# per time step variable no of features.\n",
    "\"\"\"\n",
    "Padding = \n",
    "    static padding = using FIFOQueue\n",
    "    dynamic padding = using tf.train.next_batch(..,dynamic_pad=True)\n",
    "    Bucketting = tf.contrib.training.bucket_by_sequence_length(..,dynamic_pad=True)\n",
    "    \n",
    "Goal : Handle sequences of unknown lengths\n",
    "tf.while_loop = dynamic loops and supports backprop grad descent\n",
    "tf.while_loop(cond_fn,body_fn,loop_vars,)\n",
    "\n",
    "But now to be able to process Slices of Tensors, use tf.TensorArray()\n",
    "num_rows = matrix.shape[0]\n",
    "ta = tf.TensorArray(tf.float32,size=num_rows)\n",
    "loadedMatrix = ta.unstack(matrix)\n",
    "read_row = loadedMatrix.read(row_idx)\n",
    "loadedMatrix = loadedMatrix.write(row_idx,fn(read_row))\n",
    "matrix = loadedMatrix.stack()\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to processed_uniprot.tfrecord\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define how to parse the example - In the way you stored thru the recordWriter\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"tokens\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "    \"labels\": tf.FixedLenSequenceFeature([], dtype=tf.int64)\n",
    "}\n",
    "def make_example(sequence, labels):\n",
    "    # The object we return\n",
    "    ex = tf.train.SequenceExample()\n",
    "    # A non-sequential feature of our example\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature[\"length\"].int64_list.value.append(sequence_length)\n",
    "# Since only one label will be there now, take the output of last hidden layer\n",
    "    \n",
    "    # Feature lists for the two sequential features of our example\n",
    "    fl_tokens = ex.feature_lists.feature_list[\"tokens\"]\n",
    "    fl_labels = ex.feature_lists.feature_list[\"labels\"]\n",
    "    label = labels[-1]\n",
    "    for token in sequence:\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex\n",
    "filepath='processed_uniprot.tfrecord'\n",
    "# Write all examples into a TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(filepath)\n",
    "\n",
    "#     for sequence, label_sequence in zip(sequences, label_sequences):\n",
    "for i,rec in seq_records.iterrows():\n",
    "    ex = make_example(rec.seq, [rec.fam])\n",
    "    writer.write(ex.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "print(\"Wrote to {}\".format(filepath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'length': 368}, {'length': 337}, {'length': 338}, {'length': 338}]\n",
      "[{'tokens': array([17, 13,  7, 12,  1,  5, 14, 17, 12, 20, 19,  4, 12, 22,  5,  4, 23,\n",
      "        9, 12, 14, 14, 16,  5,  1, 19,  9,  3, 20, 16,  5,  7,  9, 14,  4,\n",
      "        6, 18,  1,  9,  1, 14,  6, 17,  4, 25,  8,  7, 12,  1,  5,  6, 18,\n",
      "       14,  1, 22,  1, 11,  6, 13,  1, 18, 20, 18,  7, 14, 18,  9, 20,  6,\n",
      "        4, 16,  4, 18,  9, 22, 13, 19,  7,  7,  1, 20,  7,  1,  8,  5, 22,\n",
      "       20,  1,  6,  3, 12,  1,  4, 16,  7,  5,  1,  6, 12, 22, 16,  9, 16,\n",
      "       25, 25, 16,  7,  6,  4, 18,  4, 12, 18, 23, 18, 20,  7, 22, 11, 12,\n",
      "       22, 16, 22, 13,  3,  4, 19, 19, 14, 14,  6, 22, 12, 20, 11,  5,  1,\n",
      "       12,  5,  4,  1, 25,  5, 11,  1, 18,  5,  4, 14,  9, 18, 22, 11,  7,\n",
      "       12, 12,  9, 20, 14, 16, 19, 14, 16, 12,  7, 20,  9, 13,  4, 18, 11,\n",
      "       20, 12, 18, 20, 22, 22, 19,  6,  9, 14,  5, 11, 18,  9,  8, 12, 22,\n",
      "        3,  4,  5,  9, 25,  1,  1, 20, 22,  6, 19, 17, 16,  7,  6,  9, 19,\n",
      "        9,  1,  5,  9, 12,  5,  4,  5, 20,  4,  9,  5,  3,  4, 18, 14, 12,\n",
      "       22,  8,  9, 22, 25, 19, 12, 19, 11,  4, 13,  7,  6, 16,  7,  6, 18,\n",
      "       22,  7,  9,  9, 25, 19, 25, 14,  4,  1, 22, 22, 14,  3,  1, 18, 11,\n",
      "       13, 19, 19,  6,  7, 12, 22, 19, 20, 17, 20, 17, 25, 12, 12,  1, 19,\n",
      "       13, 12, 14,  4,  4,  5,  6, 22,  5, 18,  6, 12,  1,  5, 19,  1, 11,\n",
      "       18, 12,  1, 17, 18,  6, 18, 22,  6, 20,  7,  7, 12,  1, 11, 22,  7,\n",
      "        9, 11,  3, 12, 17, 19, 14,  1,  7, 12,  6, 22, 23, 13,  4, 12, 18,\n",
      "       17, 12, 12, 11, 11, 16, 20,  6,  4, 19,  5, 20,  5, 12, 23, 11, 22,\n",
      "        9,  9,  8,  5, 22, 11,  9, 14, 22, 19, 16,  7, 25, 19,  6,  8,  3,\n",
      "       20,  5, 16,  7, 23,  6, 18, 22,  3,  6,  1]), 'labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}, {'tokens': array([13, 12,  4,  1,  6,  4, 18, 25, 16, 12, 20,  6,  7, 16, 20, 16,  9,\n",
      "        5, 11, 12,  5, 18, 12, 20,  4,  8, 12,  7,  7, 11, 22, 17, 12, 25,\n",
      "        1, 11, 18,  5,  4,  3, 14, 19,  7, 12,  1,  6,  7,  7, 14, 11, 12,\n",
      "       18, 11, 12,  5, 25,  9,  9, 16,  4,  1,  9,  1, 19,  7,  1,  4, 20,\n",
      "       12, 22, 19,  9,  7,  7, 22, 17, 19, 14,  8, 20, 18, 13, 22,  1,  1,\n",
      "       22,  1,  1, 11,  9,  7,  6, 11,  3, 18, 12, 22, 17,  5,  1, 23, 22,\n",
      "       16,  8,  5,  4,  1, 22, 25,  4, 18, 22,  7, 14,  9, 13, 12, 19, 18,\n",
      "        9, 13,  7,  1,  4, 22, 18, 12, 22,  4,  4,  7,  6,  4,  9,  7,  9,\n",
      "       18, 18, 19, 23,  5,  5,  1,  9,  5,  5, 22, 11,  1,  1,  7,  7, 11,\n",
      "       16, 25,  1,  9, 16,  1,  7,  1, 19, 22,  8, 11, 25,  7,  7, 12,  7,\n",
      "       25, 22,  7,  6,  1,  5,  5, 22, 18,  1, 17,  5,  1,  1, 12,  7,  6,\n",
      "        1,  6,  4, 25,  9, 22, 22,  3, 20, 22, 20,  7, 19, 19,  8,  1,  7,\n",
      "       13,  1, 22,  7,  6,  1, 11,  4,  7, 18,  1,  4,  8, 22,  9,  7,  9,\n",
      "        4,  1, 19,  6, 20, 16,  4, 17, 20, 18,  1, 17, 22, 12,  5,  9,  1,\n",
      "       17, 18, 20,  1,  4, 12, 22, 11, 12,  7, 18,  5, 13, 18, 16,  5,  4,\n",
      "        9, 22, 12, 22,  5,  4, 25,  1, 25, 16, 22, 25,  7, 22, 16, 19,  5,\n",
      "        5, 20, 11,  4,  1,  9, 18, 12, 22,  7, 18, 12,  5,  7, 13,  9, 20,\n",
      "        4, 16, 22, 25,  5,  7, 11, 19, 13, 17,  7, 13,  9,  4, 12, 22, 11,\n",
      "       11,  7, 25,  6, 16,  5,  7, 19, 11, 22, 12, 25,  1,  8, 12,  7,  7,\n",
      "        1, 16,  1, 12, 14,  7, 25,  7, 25,  1,  6, 18, 14,  7]), 'labels': array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])}]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# A single serialized example\n",
    "# (You can read this from a file using TFRecordReader)\n",
    "filequeue = tf.train.string_input_producer([filepath],num_epochs=num_epochs)\n",
    "reader = tf.TFRecordReader()\n",
    "_,serial_ex=reader.read(filequeue)\n",
    "\n",
    "# Parse the example (returns a dictionary of tensors)\n",
    "context_parsed, sequence_parsed = tf.parse_single_sequence_example(\n",
    "    serialized=serial_ex,\n",
    "    context_features=context_features,\n",
    "    sequence_features=sequence_features\n",
    ")\n",
    "# context_parsed is now a Tensor !\n",
    "context = tf.contrib.learn.run_n(context_parsed, n=4, feed_dict=None)\n",
    "\n",
    "#     output_dict: A dict mapping string names to tensors to run. Must all be from the same graph.\n",
    "#     feed_dict: dict of input values to feed each run.\n",
    "#     restore_checkpoint_path: A string containing the path to a checkpoint to restore.\n",
    "#     n: Number of times to repeat.\n",
    "\n",
    "print(context)\n",
    "sequence = tf.contrib.learn.run_n(sequence_parsed, n=2, feed_dict=None)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={'someKey':tf.Variable(5)}\n",
    "b=tf.Variable([5])\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# print(tf.contrib.learn.run_n(a,n=4,feed_dict=None))\n",
    "# tf.Session().run([init,a])\n",
    "a = tf.Print(b[-1],[b])\n",
    "a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
