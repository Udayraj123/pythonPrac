{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import string\n",
    "import pickle\n",
    "import tempfile\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from nltk.util import ngrams\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configuration -\n",
    "num_ex = 300000\n",
    "famTHR = 200*num_ex/500000\n",
    "numgrams= 3 # for Glove\n",
    "min_after_dequeue = 10000*num_ex/500000\n",
    "num_epochs=1\n",
    "batch_size = 1000\n",
    "dummy = \"end end end end end\"\n",
    "\n",
    "alphabet =list(string.ascii_uppercase)\n",
    "alphabet = ['0']+alphabet\n",
    "vocab_size = len(alphabet)\n",
    "filter_file='data/filtered_seqs_all.csv'\n",
    "corpus_file='data/filtered_seq_corpus.txt'\n",
    "\n",
    "num_classes_file='uniprot_num_classes'\n",
    "\n",
    "# filepath='processed_uniprot.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printbuf(x):\n",
    "    sys.stdout.write(x)\n",
    "    sys.stdout.write('\\r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file data/uniprot_num_classes.pkl\n",
      "Loaded filtered_seqs\n"
     ]
    }
   ],
   "source": [
    "def save_obj(obj, name ,overwrite=1):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\tif(overwrite==1 and os.path.exists(name)):\n",
    "\t\treturn [];\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print('File saved to '+filename)\n",
    "\n",
    "def load_obj(name):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\t# if(not os.path.exists(name)):\n",
    "\t# \treturn [];\n",
    "\twith open('data/' + name + '.pkl', 'rb') as f:\n",
    "\t\tprint('Loaded file '+filename)\n",
    "\t\treturn pickle.load(f)\n",
    "\n",
    "def saveFilteredData():\n",
    "    fam='Cross-reference (Pfam)'\n",
    "    seq = 'Sequence'\n",
    "    main =pd.read_table(\"./data/uniprot-all.tab.gz\", sep='\\t',nrows=num_ex)\n",
    "    main[fam]= main[fam].apply(lambda x:str(x).split(';')[0])\n",
    "    col = (main[fam]!='nan') & (main[fam].isnull()==False) & (main[seq].isnull()==False)\n",
    "    main = main[col]\n",
    "    alphafam = list(main[fam].unique())\n",
    "    alphafam = [-1]+alphafam\n",
    "    num_classes = len(alphafam)\n",
    "    seq_records = main[[seq,fam,'Length']]\n",
    "    seq_records.columns=['seq','fam','length']\n",
    "    #All the heavy jobs here - takes upto 92 seconds\n",
    "    seq_records['seq']= seq_records['seq'].apply(lambda seq : list(map(lambda x : alphabet.index(x),list(seq))))\n",
    "    seq_records['fam']= seq_records['fam'].apply(lambda x : alphafam.index(x))\n",
    "    counts = seq_records.groupby(['fam']).size().reset_index(name='counts')\n",
    "    #10801 -> 9254 families\n",
    "    counts = counts[ (counts['counts']>=famTHR) & (counts['counts'] < 3400) ]\n",
    "    #608 -> 555 families having count between 200 and 3400\n",
    "    filtered_fams = list(counts['fam'])\n",
    "    #323162 -> 302907 examples total satisfy it.\n",
    "    filtered_seqs = seq_records[ seq_records['fam'].isin(filtered_fams)]\n",
    "    filtered_seqs = filtered_seqs.sort_values('length')\n",
    "#     Save for future use\n",
    "    save_obj(num_classes,num_classes_file)\n",
    "    filtered_seqs.to_csv(filter_file,index=False)\n",
    "\n",
    "    # # takes 3 seconds !\n",
    "    # gb = seq_records.groupby('fam')    \n",
    "    # family_wise_db = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "if os.path.exists(filter_file):\n",
    "    num_classes=load_obj(num_classes_file)\n",
    "    filtered_seqs = pd.read_csv(filter_file,nrows=num_ex)\n",
    "    print(\"Loaded filtered_seqs\")\n",
    "else:\n",
    "    print('File not found, preparing data again')\n",
    "    p = time()\n",
    "    saveFilteredData()\n",
    "    print('time taken: ',time() - p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating corpus...\n",
      "Wrote corpus to file\n",
      "('time taken: ', 581.1159410476685)\n"
     ]
    }
   ],
   "source": [
    "# A function that generates words of 3 grams from the given list of sequences\n",
    "# Then returns without disturbing their 'context'\n",
    "def func(x):\n",
    "    if(x <0 or x>len(alphabet)):\n",
    "        print(\"IndeX\",x)\n",
    "    print(x)\n",
    "    return alphabet[x]\n",
    "\n",
    "def genCorpus(filtered_seqs,n=3):\n",
    "#     seq is already a string(list of integers) -\n",
    "    total = filtered_seqs['seq'].size    \n",
    "    print('generating corpus : ')\n",
    "    with open(corpus_file,'a') as f:\n",
    "        i =0\n",
    "        for rec in filtered_seqs.iterrows():\n",
    "            i+=1\n",
    "            grams = ngrams(eval(rec[1].seq),n)\n",
    "            f.write(' '.join([''.join(map(lambda x :alphabet[x],g)) for g in grams]))\n",
    "            f.write(' ')\n",
    "            perc = (i*100)/total\n",
    "            printbuf('%d %% completed' % perc)\n",
    "    print(\"Wrote corpus to file\")\n",
    "\n",
    "if os.path.exists(corpus_file):\n",
    "    with open(corpus_file,'r') as f:\n",
    "        corpus = f.read()\n",
    "        print('loaded corpus')\n",
    "else :\n",
    "    p = time()\n",
    "    with open(corpus_file,'w') as f:\n",
    "        f.write('')\n",
    "    genCorpus(filtered_seqs,numgrams)\n",
    "    print('time taken: ',time() - p)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RnnForPfcModelOne:\n",
    "#   @profile\n",
    "    def __init__(self, \n",
    "        batch_size,\n",
    "        num_classes = 549, \n",
    "        hidden_units=100,\n",
    "        learning_rate=0.01,\n",
    "         logs_path='/tmp/tensorflow/logs'\n",
    "                ):\n",
    "        global vocab_size\n",
    "        # batch_size * no_of_time_steps * vocab_size _/\n",
    "        self.weights = tf.Variable(tf.random_uniform(shape=[hidden_units, num_classes], maxval=1))\n",
    "        self.biases = tf.Variable(tf.random_uniform(shape=[num_classes]))\n",
    "        self.rnn_fcell = rnn.BasicLSTMCell(num_units = hidden_units, \n",
    "                                           forget_bias = 1.0,\n",
    "                                           activation = tf.tanh)\n",
    "        # self.len_data taken from feed_dict\n",
    "        self.len_data = tf.placeholder(tf.uint8, [batch_size],name=\"Lengths\")\n",
    "        # self.x_input taken from feed_dict\n",
    "        self.x_input = tf.placeholder(tf.uint8, [None, None], name = 'x_ip') # batch_size * no_of_time_steps _/\n",
    "        # self.x_input_o takes self.x_input\n",
    "        self.x_input_o = tf.one_hot(indices = self.x_input, \n",
    "            depth = vocab_size,\n",
    "            on_value = 1.0,\n",
    "            off_value = 0.0,\n",
    "            axis = -1)\n",
    "        # self.outputs takes self.x_input_o & len_data\n",
    "        with tf.name_scope('Model'):\n",
    "            self.outputs, self.states = tf.nn.dynamic_rnn(self.rnn_fcell,\n",
    "                                                      self.x_input_o,\n",
    "                                                      sequence_length = self.len_data,\n",
    "                                                      dtype = tf.float32)\n",
    "        \n",
    "        # outputs of shape batch_size * no_of_time_steps * vocab_size\n",
    "        # output at time t i.e. the last output\n",
    "        self.outputs_t = tf.reshape(self.outputs[:, -1, :], [-1, hidden_units])\n",
    "        # The single layer NN to classify - takes outputs_t\n",
    "        self.y_predicted = tf.matmul(self.outputs_t, self.weights) + self.biases\n",
    "        \n",
    "        \n",
    "        # self.y_input taken from feed_dict batch_size *1\n",
    "        self.y_input = tf.placeholder(tf.uint8, [batch_size], name = 'y_ip')\n",
    "        # self.y_input_o takes y_input\n",
    "        self.y_input_o = tf.one_hot(indices = self.y_input, \n",
    "                                    depth = num_classes,\n",
    "                                    on_value = 1.0,\n",
    "                                    off_value = 0.0,\n",
    "                                    axis = -1)\n",
    "        \n",
    "        # self.loss takes one hot y and y_predicted\n",
    "        with tf.name_scope('Loss'):\n",
    "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.y_predicted, labels=self.y_input_o)\n",
    "        #y_predicted and y_input_o shud be of same size = batch_size * num_classes\n",
    "        # define optimizer and trainer\n",
    "        with tf.name_scope('Trainer'):\n",
    "            self.trainer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.get_equal = tf.equal(tf.argmax(self.y_input_o, 1), tf.argmax(self.y_predicted, 1))\n",
    "        with tf.name_scope('Accuracy'):\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(self.get_equal, tf.float32))\n",
    "        \n",
    "        self.summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n",
    "        # Create a summary to monitor cost tensor\n",
    "        tf.summary.scalar(\"loss\", self.loss)\n",
    "        # Create a summary to monitor accuracy tensor\n",
    "        tf.summary.scalar(\"accuracy\", self.accuracy)\n",
    "        # Merge all summaries into a single op - to pass into sess.run\n",
    "        self.merged_summary_op = tf.summary.merge_all()\n",
    "\n",
    "#   @profile\n",
    "    def predict(self, x, y, len_data):\n",
    "        result = self.sess.run(self.y_predicted, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def optimize(self, x, y, len_data,summary_index):\n",
    "        c, summary = self.sess.run([self.loss, self.merged_summary_op],\n",
    "                              feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        self.summary_writer.add_summary(summary, summary_index)\n",
    "\n",
    "        self.sess.run(self.trainer, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        return c\n",
    "\n",
    "#   @profile\n",
    "    def cross_validate(self, x, y, len_data):\n",
    "        result = self.sess.run(self.accuracy, feed_dict={self.x_input:x, self.y_input:y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def close_summary_writer(self):\n",
    "        self.summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def pad(x,max_len):\n",
    "    return np.lib.pad(x,(0,max_len - len(x)),'constant',constant_values=(-1,0))\n",
    "\n",
    "class SimpleDataIterator():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.size = len(self.df)\n",
    "        self.epochs = 0\n",
    "        self.batch_no = 0\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
    "        self.cursor = 0\n",
    "\n",
    "class PaddedDataIterator(SimpleDataIterator):\n",
    "    def next_batch(self, n):\n",
    "        if self.cursor + n > self.size:\n",
    "            self.epochs += 1\n",
    "            self.batch_no=0\n",
    "            self.shuffle()\n",
    "        res = self.df.iloc[self.cursor:self.cursor+n]\n",
    "        self.cursor += n\n",
    "        self.batch_no+=1\n",
    "\n",
    "        # Pad sequences with 0s so they are all the same length\n",
    "        maxlen = max(res['length'])\n",
    "        res['seq'] = map(lambda x:pad(x,maxlen),res['seq'])\n",
    "        return res\n",
    "\n",
    "it = PaddedDataIterator(filtered_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "runonce=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if(not runonce):\n",
    "    model = RnnForPfcModelOne(batch_size,num_classes=num_classes)\n",
    "    runonce=1\n",
    "    \n",
    "while(it.epochs != num_epochs):\n",
    "    df = it.next_batch(batch_size)\n",
    "    batch_x, batch_y, len_data = map(list,(df['seq'],df['fam'],df['length']))\n",
    "    cost = model.optimize(batch_x, batch_y, len_data,it.batch_no + batch_size*it.epochs)\n",
    "    accuracy_known = model.cross_validate(batch_x,batch_y,len_data)\n",
    "    print(\"Iteration number, batch number, Cost : \", it.epochs, it.batch_no,cost,\n",
    "          \" Training data accuracy : \", accuracy_known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
