{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "1 2 88 42 99\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    1 2 88 42 99\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Plan : Start small. \n",
    "First, practice with sequenceExample\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pickle\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy as np\n",
    "\n",
    "def save_obj(obj, name ,overwrite=1):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\tif(overwrite==1 and os.path.exists(name)):\n",
    "\t\treturn [];\n",
    "\twith open(filename, 'wb') as f:\n",
    "\t\tpickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "\tfilename='data/'+ name + '.pkl';\n",
    "\t# if(not os.path.exists(name)):\n",
    "\t# \treturn [];\n",
    "\twith open('data/' + name + '.pkl', 'rb') as f:\n",
    "\t\treturn pickle.load(f)\n",
    "\n",
    "main =pd.read_table(\"./data/uniprot-all-pfam.tab.gz\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_records = main[['Sequence','Cross-reference (Pfam)']]\n",
    "seq_records.columns=['seq','fam']\n",
    "col = seq_records['fam'].isnull()==False\n",
    "seq_records = seq_records[col]\n",
    "# \n",
    "num_ex = 10\n",
    "sample = seq_records.sample(num_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample['fam']= sample['fam'].apply(lambda x:str(x).split(';')[0])\n",
    "seq_records['fam']= seq_records['fam'].apply(lambda x:str(x).split(';')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_records=sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'M' has type str, but expected one of: int, long",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-bc4fbb22f44d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrecord\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq_records\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'seq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fam'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-bc4fbb22f44d>\u001b[0m in \u001b[0;36mmake_example\u001b[0;34m(sequence, labels)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfl_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_lists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mfl_tokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mfl_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'M' has type str, but expected one of: int, long"
     ]
    }
   ],
   "source": [
    "sequences = [[1, 2, 3], [4, 5, 1], [1, 2]]\n",
    "label_sequences = [[0, 1, 0], [1, 0, 0], [1, 1]]\n",
    "\n",
    "def make_example(sequence, labels):\n",
    "    # The object we return\n",
    "    ex = tf.train.SequenceExample()\n",
    "    # A non-sequential feature of our example\n",
    "    sequence_length = len(sequence)\n",
    "    ex.context.feature[\"length\"].int64_list.value.append(sequence_length)\n",
    "    # Feature lists for the two sequential features of our example\n",
    "    fl_tokens = ex.feature_lists.feature_list[\"tokens\"]\n",
    "    fl_labels = ex.feature_lists.feature_list[\"labels\"]\n",
    "    for token, label in zip(sequence, labels):\n",
    "        fl_tokens.feature.add().int64_list.value.append(token)\n",
    "        fl_labels.feature.add().int64_list.value.append(label)\n",
    "    return ex\n",
    "\n",
    "# Write all examples into a TFRecords file\n",
    "with tempfile.NamedTemporaryFile() as fp:\n",
    "    writer = tf.python_io.TFRecordWriter(fp.name)\n",
    "    \n",
    "    for i,record in seq_records.iterrows():\n",
    "        ex = make_example(record['seq'], record['fam'])\n",
    "        writer.write(ex.SerializeToString())\n",
    "    \n",
    "    writer.close()\n",
    "    print(\"Wrote to {}\".format(fp.name))\n",
    "\"\"\"\n",
    "SequenceData can be stored into TFRecords that is tf's own storage. \n",
    "It is easier to handle and reusable\n",
    "& it follows the protocol buffer format already\n",
    "\"\"\"\n",
    "\n",
    "# efficient storage of the sequences\n",
    "# per time step variable no of features.\n",
    "\"\"\"\n",
    "Padding = \n",
    "    static padding = using FIFOQueue\n",
    "    dynamic padding = using tf.train.next_batch(..,dynamic_pad=True)\n",
    "    Bucketting = tf.contrib.training.bucket_by_sequence_length(..,dynamic_pad=True)\n",
    "    \n",
    "Goal : Handle sequences of unknown lengths\n",
    "tf.while_loop = dynamic loops and supports backprop grad descent\n",
    "tf.while_loop(cond_fn,body_fn,loop_vars,)\n",
    "\n",
    "But now to be able to process Slices of Tensors, use tf.TensorArray()\n",
    "num_rows = matrix.shape[0]\n",
    "ta = tf.TensorArray(tf.float32,size=num_rows)\n",
    "loadedMatrix = ta.unstack(matrix)\n",
    "read_row = loadedMatrix.read(row_idx)\n",
    "loadedMatrix = loadedMatrix.write(row_idx,fn(read_row))\n",
    "matrix = loadedMatrix.stack()\n",
    "\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
