{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "# Configuration -\n",
    "num_ex = 500#000\n",
    "famTHR = 200*num_ex/500000\n",
    "min_after_dequeue = 1#10000*num_ex/500000\n",
    "num_epochs=1\n",
    "batch_size = 10\n",
    "runonce=0\n",
    "filepath='processed_uniprot.tfrecord'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[2]:\n",
    "\n",
    "\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "import pickle\n",
    "import tempfile\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import matplotlib.pyplot as plot\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "def save_obj(obj, name ,overwrite=1):\n",
    "    filename='data/'+ name + '.pkl';\n",
    "    if(overwrite==1 and os.path.exists(name)):\n",
    "        return [];\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    filename='data/'+ name + '.pkl';\n",
    "    # if(not os.path.exists(name)):\n",
    "    #   return [];\n",
    "    with open('data/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "fam='Cross-reference (Pfam)'\n",
    "seq = 'Sequence'\n",
    "\n",
    "main =pd.read_table(\"./data/uniprot-all.tab.gz\", sep='\\t',nrows=num_ex)\n",
    "\n",
    "main[fam]= main[fam].apply(lambda x:str(x).split(';')[0])\n",
    "col = (main[fam]!='nan') & (main[fam].isnull()==False) & (main[seq].isnull()==False)\n",
    "main = main[col]\n",
    "alphafam = list(main[fam].unique())\n",
    "num_classes = len(alphafam)+1 \n",
    "alphabet =list(string.ascii_uppercase)\n",
    "vocab_size = 1+len(alphabet)\n",
    "\n",
    "\n",
    "# In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq_records = main[[seq,fam,'Length']]\n",
    "seq_records.columns=['seq','fam','length']\n",
    "sample = seq_records#.sample(num_ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[4]:\n",
    "\n",
    "#All the heavy jobs here - takes upto 92 seconds\n",
    "seq_records = sample.copy()\n",
    "seq_records['seq']= seq_records['seq'].apply(lambda seq : list(map(lambda x : 1+alphabet.index(x),list(seq))))\n",
    "seq_records['fam']= seq_records['fam'].apply(lambda x : 1+alphafam.index(x))\n",
    "#added 1+ for skipping 0 index\n",
    "counts = seq_records.groupby(['fam']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[5]:\n",
    "\n",
    "\n",
    "#10801 -> 9254 families\n",
    "counts = counts[ (counts['counts']>=famTHR) & (counts['counts'] < 3400) ]\n",
    "#608 -> 555 families having count between 200 and 3400\n",
    "filtered_fams = list(counts['fam'])\n",
    "#323162 -> 302907 examples total satisfy it.\n",
    "filtered_seqs = seq_records[ seq_records['fam'].isin(filtered_fams)]\n",
    "filtered_seqs = filtered_seqs.sort_values('length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[6]:\n",
    "\n",
    "\n",
    "# takes 3 seconds !\n",
    "gb = seq_records.groupby('fam')    \n",
    "family_wise_db = [gb.get_group(x) for x in gb.groups]\n",
    "\n",
    "\n",
    "# In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# seq_records = filtered_seqs\n",
    "filtered_seqs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[8]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "x = pd.DataFrame([[1],[2],[1],[3]])\n",
    "x = x[(x==1) | (x==2)]\n",
    "x.dropna()\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "SequenceData can be stored into TFRecords that is tf's own storage. \n",
    "It is easier to handle and reusable\n",
    "& it follows the protocol buffer format already\n",
    "\"\"\"\n",
    "# efficient storage of the sequences\n",
    "# per time step variable no of features.\n",
    "\"\"\"\n",
    "Padding = \n",
    "    static padding = using FIFOQueue\n",
    "    dynamic padding = using tf.train.next_batch(..,dynamic_pad=True)\n",
    "    Bucketting = tf.contrib.training.bucket_by_sequence_length(..,dynamic_pad=True)\n",
    "    \n",
    "Goal : Handle sequences of unknown lengths\n",
    "tf.while_loop = dynamic loops and supports backprop grad descent\n",
    "tf.while_loop(cond_fn,body_fn,loop_vars,)\n",
    "\n",
    "But now to be able to process Slices of Tensors, use tf.TensorArray()\n",
    "num_rows = matrix.shape[0]\n",
    "ta = tf.TensorArray(tf.float32,size=num_rows)\n",
    "loadedMatrix = ta.unstack(matrix)\n",
    "read_row = loadedMatrix.read(row_idx)\n",
    "loadedMatrix = loadedMatrix.write(row_idx,fn(read_row))\n",
    "matrix = loadedMatrix.stack()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[9]:\n",
    "# Define how to parse the example - In the way you stored thru the recordWriter\n",
    "context_features = {\n",
    "    \"length\": tf.FixedLenFeature([], dtype=tf.int64),\n",
    "    \"label\": tf.FixedLenFeature([], dtype=tf.int64)\n",
    "}\n",
    "sequence_features = {\n",
    "    \"sequence\": tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "}\n",
    "\n",
    "def _int64_feature(value):\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "def _bytes_feature(value):\n",
    "      \"\"\"Wrapper for inserting a bytes Feature into a SequenceExample proto.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))\n",
    "\n",
    "def make2_example(sequence, label,sequence_length):\n",
    "    # The object we return\n",
    "    ex = tf.train.SequenceExample()\n",
    "    # A non-sequential feature of our example\n",
    "    ex.context.feature[\"length\"].int64_list.value.append(sequence_length)\n",
    "    ex.context.feature[\"label\"].int64_list.value.append(label)\n",
    "    # Feature lists for the two sequential features of our example\n",
    "    fl_sequence = ex.feature_lists.feature_list[\"sequence\"]\n",
    "    for token in  sequence:\n",
    "        fl_sequence.feature.add().int64_list.value.append(token)\n",
    "    return ex\n",
    "\n",
    "def make_example(sequence, label,length):\n",
    "    feature = {\n",
    "       'train/label': _int64_feature(label),\n",
    "       'train/length': _int64_feature(length),\n",
    "       'train/sequence': _bytes_feature(tf.compat.as_bytes(str(sequence)))\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example\n",
    "\n",
    "\n",
    "# Write all examples into a TFRecords file\n",
    "writer = tf.python_io.TFRecordWriter(filepath)\n",
    "\n",
    "#     for sequence, label_sequence in zip(sequences, label_sequences):\n",
    "for i,rec in filtered_seqs.iterrows():\n",
    "    ex = make_example(rec.seq, rec.fam,rec.length)\n",
    "    writer.write(ex.SerializeToString())\n",
    "\n",
    "writer.close()\n",
    "print(\"Wrote to {}\".format(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[10]:\n",
    "\n",
    "\n",
    "x =[{'length': 368,'a':'a'}, {'length': 337}, {'length': 338}, {'length': 338}]\n",
    "# map(lambda x: (x['length'],x),x)\n",
    "x[0].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[11]:\n",
    "\n",
    "\n",
    "# a={'someKey':tf.Variable(5)}\n",
    "# b=tf.Variable([5])\n",
    "# sess = tf.InteractiveSession()\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# # print(tf.contrib.learn.run_n(a,n=4,feed_dict=None))\n",
    "# # tf.Session().run([init,a])\n",
    "# tf.Print(b[-1],[b]).eval()\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[12]:\n",
    "\n",
    "\n",
    "class RnnForPfcModelOne:\n",
    "#   @profile\n",
    "    def __init__(self, \n",
    "        batch_size,\n",
    "        num_classes = 549, \n",
    "        hidden_units=100,\n",
    "        learning_rate=0.01):\n",
    "        global vocab_size\n",
    "        # batch_size * no_of_time_steps * vocab_size _/\n",
    "        self.weights = tf.Variable(tf.random_uniform(shape=[hidden_units, num_classes], maxval=1))\n",
    "        self.biases = tf.Variable(tf.random_uniform(shape=[num_classes]))\n",
    "        self.rnn_fcell = rnn.BasicLSTMCell(num_units = hidden_units, \n",
    "                                           forget_bias = 1.0,\n",
    "                                           activation = tf.tanh)\n",
    "        # self.len_data taken from feed_dict\n",
    "        self.len_data = tf.placeholder(tf.uint8, [batch_size])\n",
    "        # self.x_input taken from feed_dict\n",
    "        self.x_input = tf.placeholder(tf.uint8, [None, None], name = 'x_ip') # batch_size * no_of_time_steps _/\n",
    "        # self.x_input_o takes self.x_input\n",
    "        self.x_input_o = tf.one_hot(indices = self.x_input, \n",
    "            depth = vocab_size,\n",
    "            on_value = 1.0,\n",
    "            off_value = 0.0,\n",
    "            axis = -1)\n",
    "        # self.outputs takes self.x_input_o & len_data\n",
    "        self.outputs, self.states = tf.nn.dynamic_rnn(self.rnn_fcell,\n",
    "                                                      self.x_input_o,\n",
    "                                                      sequence_length = self.len_data,\n",
    "                                                      dtype = tf.float32)\n",
    "        \n",
    "        # outputs of shape batch_size * no_of_time_steps * vocab_size\n",
    "        # output at time t i.e. the last output\n",
    "        self.outputs_t = tf.reshape(self.outputs[:, -1, :], [-1, hidden_units])\n",
    "        # The single layer NN to classify - takes outputs_t\n",
    "        self.y_predicted = tf.matmul(self.outputs_t, self.weights) + self.biases\n",
    "        \n",
    "        \n",
    "        # self.y_input taken from feed_dict batch_size *1\n",
    "        self.y_input = tf.placeholder(tf.uint8, [batch_size], name = 'y_ip')\n",
    "        # self.y_input_o takes y_input\n",
    "        self.y_input_o = tf.one_hot(indices = self.y_input, \n",
    "                                    depth = num_classes,\n",
    "                                    on_value = 1.0,\n",
    "                                    off_value = 0.0,\n",
    "                                    axis = -1)\n",
    "        \n",
    "        # self.loss takes one hot y and y_predicted\n",
    "        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.y_predicted, labels=self.y_input_o)\n",
    "        #y_predicted and y_input_o shud be of same size = batch_size * num_classes\n",
    "        # define optimizer and trainer\n",
    "        self.trainer = tf.train.GradientDescentOptimizer(learning_rate).minimize(self.loss)\n",
    "\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        self.get_equal = tf.equal(tf.argmax(self.y_input_o, 1), tf.argmax(self.y_predicted, 1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.get_equal, tf.float32))\n",
    "        self.summary_writer = tf.summary.FileWriter('./data/graph/', graph = self.sess.graph)\n",
    "\n",
    "#   @profile\n",
    "    def predict(self, x, y, len_data):\n",
    "        result = self.sess.run(self.y_predicted, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def optimize(self, x, y, len_data):\n",
    "        self.sess.run(self.trainer, feed_dict={self.x_input: x, self.y_input: y, self.len_data:len_data})\n",
    "\n",
    "#   @profile\n",
    "    def cross_validate(self, x, y, len_data):\n",
    "        result = self.sess.run(self.accuracy, feed_dict={self.x_input:x, self.y_input:y, self.len_data:len_data})\n",
    "        return result\n",
    "\n",
    "#   @profile\n",
    "    def close_summary_writer(self):\n",
    "        self.summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[13]:\n",
    "\n",
    "\n",
    "# tf.reset_default_graph()\n",
    "#  READING DATA BACK- \n",
    "# A single serialized example\n",
    "\n",
    "\n",
    "feature = {\n",
    "            'train/sequence': tf.FixedLenFeature([], tf.string),\n",
    "           'train/length': tf.FixedLenFeature([], tf.int64),\n",
    "           'train/label': tf.FixedLenFeature([], tf.int64)\n",
    "          }\n",
    "# In[ ]:\n",
    "\n",
    "def read_my_file_format(filequeue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _,serialized_example=reader.read(filequeue)\n",
    "    features = tf.parse_single_example(serialized_example, features=feature)\n",
    "    return features['train/sequence'],features['train/label'],features['train/length']\n",
    "# In[ ]:\n",
    "\n",
    "def input_pipeline(filepath, batch_size,pad_length,min_after_dequeue = 10000, num_epochs=None):\n",
    "    filequeue = tf.train.string_input_producer([filepath],num_epochs=1)#,shuffle=True)\n",
    "    sequence, label,length = read_my_file_format(filequeue)\n",
    "    \n",
    "    sequence = tf.decode_raw(sequence, tf.uint8)\n",
    "        \n",
    "    # Cast label data into int32\n",
    "    label = tf.cast(label, tf.int32)\n",
    "    length = tf.cast(length, tf.int32)\n",
    "    sequence = tf.reshape(sequence, [pad_length])\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "\n",
    "#  You can create the batch queue using tf.train.shuffle_batch([image, label], batch_size=10, capacity=30, num_threads=1, min_after_dequeue=10)\n",
    "    example_batch, label_batch = tf.train.shuffle_batch(\n",
    "          [sequence, label], \n",
    "          # shapes=[[pad_length],[1]],\n",
    "          batch_size=batch_size, capacity=capacity,\n",
    "          min_after_dequeue=min_after_dequeue)\n",
    "    return example_batch, label_batch\n",
    "\n",
    "\n",
    "    \n",
    "#Instead of running this tensor-\n",
    "#         len_data = tf.contrib.learn.run_n(context_parsed, n=batch_size, feed_dict=None)\n",
    "# we'll run a custom tensor now\n",
    "# decode_x,decode_y,decode_len = read_and_decode_module(filepath,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In[14]:\n",
    "\n",
    "\n",
    "if(not runonce):\n",
    "    model = RnnForPfcModelOne(batch_size,num_classes=num_classes)\n",
    "    runonce=1\n",
    "\n",
    "#     output_dict: A dict mapping string names to tensors to run. Must all be from the same graph.\n",
    "#     feed_dict: dict of input values to feed each run.\n",
    "#     restore_checkpoint_path: A string containing the path to a checkpoint to restore.\n",
    "#     n: Number of times to repeat.\n",
    "\"\"\"\n",
    "This comes  with np.array([1,2,3,[4,5,6]) or dtype is wrong\n",
    "ValueError: setting an array element with a sequence.\n",
    "So - padding was the culprit\n",
    "\n",
    "\"\"\"\n",
    "# data_train, data_test, data_cv = get_data(200)\n",
    "# print(len(data_train), (len(data_test)), (len(data_cv)))\n",
    "def pad(x,max_len):\n",
    "    return np.lib.pad(x,(0,max_len - len(x)),'constant',constant_values=(-1,0))\n",
    "\n",
    "# record_iterator = tf.python_io.tf_record_iterator(path=filepath)\n",
    "# for string_record in record_iterator:\n",
    "#     example = tf.train.Example()\n",
    "#     example.ParseFromString(string_record)\n",
    "#     sequence = np.fromstring(example.features.feature['train/sequence'].bytes_list.value[0], dtype=np.uint8)\n",
    "#     label = int(example.features.feature['train/label'].int64_list.value[0])\n",
    "#     length = int(example.features.feature['train/length'].int64_list.value[0])\n",
    "#     print(sequence,length,label)\n",
    "    # debug=input()\n",
    "        \n",
    "pad_length=1000 # Get this from a file now\n",
    "with tf.Session() as sess:\n",
    "    batch_x, batch_y = input_pipeline(filepath,batch_size,pad_length,min_after_dequeue,num_epochs)\n",
    "    # Initialize all global and local variables\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    sess.run(init_op)\n",
    "\n",
    "    # Create a coordinator and run all QueueRunner objects\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "    for batch_index in range(5):\n",
    "        print('sess running')\n",
    "        seq, lbl = sess.run([batch_x, batch_y ])\n",
    "        print(seq,lbl)\n",
    "\n",
    "    # Stop the threads\n",
    "    coord.request_stop()\n",
    "\n",
    "    # Wait for threads to stop\n",
    "    coord.join(threads)\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     no_of_batches = num_ex // batch_size\n",
    "#     # queue for the data\n",
    "#     for batch_no in range(no_of_batches):\n",
    "# #         batch_x,batch_y,len_data = sess.run([decode_x,decode_y,decode_len])\n",
    "#         # x_data = tf.contrib.learn.run_n(context_parsed, n=batch_size, feed_dict=None)\n",
    "#         # y_data = tf.contrib.learn.run_n(sequence_parsed, n=batch_size, feed_dict=None)\n",
    "#         print('sess running')\n",
    "#         seq, lbl = model.sess.run([batch_x, batch_y ])\n",
    "#         print(seq,lbl)\n",
    "#         debug=input()\n",
    "#         model.optimize(batch_x,batch_y,len_data)\n",
    "#         accuracy_known = model.cross_validate(batch_x,batch_y,len_data)\n",
    "#         print(\"Iteration number, batch number : \", epoch, batch_no,\" Training data accuracy : \", accuracy_known)\n",
    "\n",
    "# model.close_summary_writer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
