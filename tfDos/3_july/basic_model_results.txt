Report of first basic model with tanh activation, 300 units, LSTM cells, bidirectional RNN (everythin in terms of accuracy).
TL;DR
First  -> the ouput without first iteration is very bad. 0.0219085 on test and 0.0217286 on first batch.
Second -> there is significant increase in performance with iteration, 0.705809 on test 0.699257 on first batch. 
Third  -> thereafter nothing changes, probably suffering from the problem of vanishing gradients.
Fourth -> changed the batch that first goes as train data to the model, so now instead of 0-100th train data, 
          101-200th train data is going as input. Then again the same thing happens with same accuracy. 


Conclusions : 
Online discussion forums say that RNN of tensorflow has very confusing documentation and
(https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/74BrHpM76Q4) 
terms which are used differ from in general like cell. Will need to go through the basic understanding of the model 
and RNN implementation of tensorflow.
Solution : If we are only using the API, and (if) we are able to perform better, maybe we can code the enitre thing in theano and check ?
Check for problem of vanishing gradient, will need to learn how to debug it.
Check how the model performs on 6133filtered data and then on 53 data.



/usr/bin/python3.5 /home/sud/PycharmProjects/tensorFlowProj/LSTMforPSSP/lstmForPSSP.py
1.1.0
(6133, 39900)
(6133, 700, 57)
(5600, 700, 57)
(277, 700, 57)
(256, 700, 57)
Train data residues shape :  (5600, 700, 21)
Train data secondary structue :  (5600, 700, 8)
Train data n and c terminals :  (5600, 700, 2)
Train data relative and absolute solvability :  (5600, 700, 2)
Train data sequence profile :  (5600, 700, 22)
2017-06-02 16:31:35.312221: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 16:31:35.312252: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 16:31:35.312258: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 16:31:35.312262: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2017-06-02 16:31:35.312266: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
Successfully created the model
0 td -  0.0219085
Iteration number & batch number:  0 0
Accuracy on next-to-be-trained batch :  0.0217286
Accuracy obtained after training     :  0.699257
Iteration number & batch number:  1 1
Accuracy on next-to-be-trained batch :  0.689871
Accuracy obtained after training     :  0.689871
Iteration number & batch number:  2 2
Accuracy on next-to-be-trained batch :  0.681714
Accuracy obtained after training     :  0.681714
Iteration number & batch number:  3 3
Accuracy on next-to-be-trained batch :  0.695557
Accuracy obtained after training     :  0.695557
Iteration number & batch number:  4 4
Accuracy on next-to-be-trained batch :  0.708329
Accuracy obtained after training     :  0.708329
Iteration number & batch number:  5 5
Accuracy on next-to-be-trained batch :  0.735586
Accuracy obtained after training     :  0.735586
Iteration number & batch number:  6 6
Accuracy on next-to-be-trained batch :  0.712514
Accuracy obtained after training     :  0.712514
Iteration number & batch number:  7 7
Accuracy on next-to-be-trained batch :  0.702171
Accuracy obtained after training     :  0.702171
Iteration number & batch number:  8 8
Accuracy on next-to-be-trained batch :  0.744786
Accuracy obtained after training     :  0.744786
Iteration number & batch number:  9 9
Accuracy on next-to-be-trained batch :  0.673029
Accuracy obtained after training     :  0.673029
10 td -  0.705809
Iteration number & batch number:  10 10
Accuracy on next-to-be-trained batch :  0.663829
Accuracy obtained after training     :  0.663829
Iteration number & batch number:  11 11
Accuracy on next-to-be-trained batch :  0.681943
Accuracy obtained after training     :  0.681943
Iteration number & batch number:  12 12
Accuracy on next-to-be-trained batch :  0.721657
Accuracy obtained after training     :  0.721657
Iteration number & batch number:  13 13
Accuracy on next-to-be-trained batch :  0.714357
Accuracy obtained after training     :  0.714357
Iteration number & batch number:  14 14
Accuracy on next-to-be-trained batch :  0.706371
Accuracy obtained after training     :  0.706371
Iteration number & batch number:  15 15
Accuracy on next-to-be-trained batch :  0.716971
Accuracy obtained after training     :  0.716971
Iteration number & batch number:  16 16
Accuracy on next-to-be-trained batch :  0.723729
Accuracy obtained after training     :  0.723729
Iteration number & batch number:  17 17
Accuracy on next-to-be-trained batch :  0.691986
Accuracy obtained after training     :  0.691986
Iteration number & batch number:  18 18
Accuracy on next-to-be-trained batch :  0.697886
Accuracy obtained after training     :  0.697886
Iteration number & batch number:  19 19
Accuracy on next-to-be-trained batch :  0.673343
Accuracy obtained after training     :  0.673343
20 td -  0.705809
Iteration number & batch number:  20 20
Accuracy on next-to-be-trained batch :  0.699557
Accuracy obtained after training     :  0.699557
Iteration number & batch number:  21 21
Accuracy on next-to-be-trained batch :  0.675586
Accuracy obtained after training     :  0.675586
Iteration number & batch number:  22 22
Accuracy on next-to-be-trained batch :  0.738429
Accuracy obtained after training     :  0.738429
Iteration number & batch number:  23 23
Accuracy on next-to-be-trained batch :  0.674671
Accuracy obtained after training     :  0.674671
Iteration number & batch number:  24 24
Accuracy on next-to-be-trained batch :  0.716243
Accuracy obtained after training     :  0.716243
Iteration number & batch number:  25 25
Accuracy on next-to-be-trained batch :  0.682257
Accuracy obtained after training     :  0.682257
Iteration number & batch number:  26 26
Accuracy on next-to-be-trained batch :  0.702557
Accuracy obtained after training     :  0.702557
Iteration number & batch number:  27 27
Accuracy on next-to-be-trained batch :  0.694943
Accuracy obtained after training     :  0.694943
Iteration number & batch number:  28 28
Accuracy on next-to-be-trained batch :  0.709943
Accuracy obtained after training     :  0.709943
Iteration number & batch number:  29 29
Accuracy on next-to-be-trained batch :  0.692614
Accuracy obtained after training     :  0.692614
30 td -  0.705809
Iteration number & batch number:  30 30
Accuracy on next-to-be-trained batch :  0.693457
Accuracy obtained after training     :  0.693457
Iteration number & batch number:  31 31
Accuracy on next-to-be-trained batch :  0.672443
Accuracy obtained after training     :  0.672443
Iteration number & batch number:  32 32
Accuracy on next-to-be-trained batch :  0.690557
Accuracy obtained after training     :  0.690557
Iteration number & batch number:  33 33
Accuracy on next-to-be-trained batch :  0.702814
Accuracy obtained after training     :  0.702814
Iteration number & batch number:  34 34
Accuracy on next-to-be-trained batch :  0.753714
Accuracy obtained after training     :  0.753714
Iteration number & batch number:  35 35
Accuracy on next-to-be-trained batch :  0.7064
Accuracy obtained after training     :  0.7064
Iteration number & batch number:  36 36
Accuracy on next-to-be-trained batch :  0.722671
Accuracy obtained after training     :  0.722671
Iteration number & batch number:  37 37
Accuracy on next-to-be-trained batch :  0.692914
Accuracy obtained after training     :  0.692914
Iteration number & batch number:  38 38
Accuracy on next-to-be-trained batch :  0.725429
Accuracy obtained after training     :  0.725429
Iteration number & batch number:  39 39
Accuracy on next-to-be-trained batch :  0.677657
Accuracy obtained after training     :  0.677657
40 td -  0.705809
Iteration number & batch number:  40 40
Accuracy on next-to-be-trained batch :  0.713586
Accuracy obtained after training     :  0.713586
Iteration number & batch number:  41 41
Accuracy on next-to-be-trained batch :  0.705886
Accuracy obtained after training     :  0.705886
Iteration number & batch number:  42 42
Accuracy on next-to-be-trained batch :  0.7153
Accuracy obtained after training     :  0.7153
Iteration number & batch number:  43 43
Accuracy on next-to-be-trained batch :  0.717886
Accuracy obtained after training     :  0.717886
Iteration number & batch number:  44 44
Accuracy on next-to-be-trained batch :  0.716543
Accuracy obtained after training     :  0.716543
Iteration number & batch number:  45 45
Accuracy on next-to-be-trained batch :  0.681671
Accuracy obtained after training     :  0.681671
Iteration number & batch number:  46 46
Accuracy on next-to-be-trained batch :  0.673314
Accuracy obtained after training     :  0.673314
Iteration number & batch number:  47 47
Accuracy on next-to-be-trained batch :  0.723814
Accuracy obtained after training     :  0.723814
Iteration number & batch number:  48 48
Accuracy on next-to-be-trained batch :  0.691643
Accuracy obtained after training     :  0.691643
Iteration number & batch number:  49 49
Accuracy on next-to-be-trained batch :  0.727886
Accuracy obtained after training     :  0.727886
50 td -  0.705809
Iteration number & batch number:  50 50
Accuracy on next-to-be-trained batch :  0.689857
Accuracy obtained after training     :  0.689857
Iteration number & batch number:  51 51
Accuracy on next-to-be-trained batch :  0.688229
Accuracy obtained after training     :  0.688229
Iteration number & batch number:  52 52
Accuracy on next-to-be-trained batch :  0.6907
Accuracy obtained after training     :  0.6907
Iteration number & batch number:  53 53
Accuracy on next-to-be-trained batch :  0.692814
Accuracy obtained after training     :  0.692814
Iteration number & batch number:  54 54
Accuracy on next-to-be-trained batch :  0.712543
Accuracy obtained after training     :  0.712543
Iteration number & batch number:  55 55
Accuracy on next-to-be-trained batch :  0.710743
Accuracy obtained after training     :  0.710743
Iteration number & batch number:  56 0
Accuracy on next-to-be-trained batch :  0.699257
Accuracy obtained after training     :  0.699257
Iteration number & batch number:  57 1
Accuracy on next-to-be-trained batch :  0.689871
Accuracy obtained after training     :  0.689871
Iteration number & batch number:  58 2
Accuracy on next-to-be-trained batch :  0.681714
Accuracy obtained after training     :  0.681714
Iteration number & batch number:  59 3
Accuracy on next-to-be-trained batch :  0.695557
Accuracy obtained after training     :  0.695557
60 td -  0.705809
Iteration number & batch number:  60 4
Accuracy on next-to-be-trained batch :  0.708329
Accuracy obtained after training     :  0.708329




Code used :
# Trying to reproduce results from
# paper :    arXiv : 1412.7828v2 [q-bio.QM] 4 Jan 2015
# Experiment name : Protein secondary structure prediction using
# LSTM networks.

# Model :
#  -- Standard stacked bidirectional LSTM with 3 layers.
#  -- (300 or 500) LSTM units in each layer
#  -- There is a FFN between h_rec and h with a skip connection. h_rec = ffn(h) + h
#  -- FFN is a two layer ReLU network with 300 or 500 units,
#  -- Introduce a FFN to combine output from forward and backward RNN
#  -- Has a ReLU with 200 or 400 hidden units.
#  -- The concatenation is regularized with 50% dropout.

import tensorflow as tf
import numpy as np
from tensorflow.contrib import rnn
import pandas as pd

print(tf.__version__)

data = np.load('./data/cullpdb+profile_6133.npy.gz')
print(data.shape)
data = np.reshape(data, [6133, 700, 57])
print(data.shape)

# print(data.info())
train_data = data[:5600, :]
cv_data = data[5600:5877, :]
test_data = data[5877:6133, :]

print(train_data.shape)
print(cv_data.shape)
print(test_data.shape)

"""
Source : arXiv:1403.1347v1  [q-bio.QM]  6 Mar 2014
:Deep Supervised and Convolutional Generative Stochastic Network for Protein Secondary Structure Prediction

The resulting training data including both feature and la-
bels has 57 channels (22 for PSSM, 22 for sequence, 2 for
terminals,  8  for  secondary  structure  labels,  2  for  solvent
accessibility  labels),  and  the  overall  channel  size  is  700.
"""

"""
Source : http://www.princeton.edu/~jzthree/datasets/ICML2014/dataset_readme.txt
It is currently in numpy format as a (N protein x k features) matrix. You can reshape it to (N protein x 700 amino acids x 57 features) first.

The 57 features are:
"[0,22): amino acid residues, with the order of 'A', 'C', 'E', 'D', 'G', 'F',
'I', 'H', 'K', 'M', 'L', 'N', 'Q', 'P', 'S', 'R', 'T', 'W', 'V', 'Y', 'X','NoSeq'"
"[22,31): Secondary structure labels, with the sequence of 'L', 'B', 'E', 'G', 'I', 'H',
'S', 'T','NoSeq'"
"[31,33): N- and C- terminals;"
"[33,35): relative and absolute solvent accessibility, used only for training.
(absolute accessibility is thresholded at 15; relative accessibility is normalized by the largest accessibility
value in a protein and thresholded at 0.15; original solvent accessibility is computed by DSSP)"
"[35,57): sequence profile. Note the order of amino acid residues is ACDEFGHIKLMNPQRSTVWXY and
it is different from the order for amino acid residues"

The last feature of both amino acid residues and secondary structure labels just mark end of the protein sequence.
"[22,31) and [33,35) are hidden during testing."


"The dataset division for the first ""cullpdb+profile_6133.npy.gz"" dataset is"
"[0,5600) training"
"[5605,5877) test "
"[5877,6133) validation"
"""
# Split the train data
train_data_residues = train_data[:, :,  0:21]
train_data_secstruc = train_data[:, :, 22:30]
train_data_nctermin = train_data[:, :, 31:33]
train_data_rlabsolv = train_data[:, :, 33:35]
train_data_sequepro = train_data[:, :, 35:57]

# Checking shapes
print("Train data residues shape : ", train_data_residues.shape)
print("Train data secondary structue : ",train_data_secstruc.shape)
print("Train data n and c terminals : ", train_data_nctermin.shape)
print("Train data relative and absolute solvability : ", train_data_rlabsolv.shape)
print("Train data sequence profile : ", train_data_sequepro.shape)

train_data_input = train_data[:, :, np.r_[0:21, 36:57]]
train_data_otput = train_data[:, :, 23:31]
test_data_input = test_data[:, :, np.r_[0:21, 36:57]]
test_data_otput = test_data[:, :, 23:31]
# Checking shapes
# print("Train data input  shape : ", train_data_input.shape)
# print("Train data output shape : ", train_data_otput.shape)

learning_rate = 0.1
n_epochs = 1000
num_classes = 8
hidden_units = 300

class BrnnForPssp():

    def __init__(self, learning_rate, num_classes, hidden_units):

        # Initialize data and variables
        self.weights = tf.Variable(tf.random_uniform([hidden_units*2, num_classes], minval=-0.5, maxval=0.5))
        self.biases  = tf.Variable(tf.zeros([num_classes]))
        self.x = tf.placeholder("float", [None, 700, 42])
        self.y = tf.placeholder("float", [None, 700, 8])

        # Do the prediction

        # Remember to change activation to ReLU
        self.fw_rnn_cell1 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        self.fw_rnn_cell2 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        self.fw_rnn_cell3 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        self.bw_rnn_cell1 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        self.bw_rnn_cell2 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        self.bw_rnn_cell3 = rnn.LSTMCell(hidden_units, forget_bias=1.0)
        # self.fw_rnn_cells = [self.fw_rnn_cell1, self.fw_rnn_cell2, self.fw_rnn_cell3]
        # self.bw_rnn_cells = [self.bw_rnn_cell1, self.bw_rnn_cell2, self.bw_rnn_cell3]
        self.fw_rnn_cells = [self.fw_rnn_cell1]
        self.bw_rnn_cells = [self.bw_rnn_cell1]
        self.outputs, self.states_fw, self.states_bw = rnn.stack_bidirectional_dynamic_rnn(
                                                            self.fw_rnn_cells,
                                                            self.bw_rnn_cells,
                                                            self.x,
                                                            dtype=tf.float32)
        # self.output.shape is (?, 700, 600)
        self.outputs_reshaped = tf.reshape(self.outputs, [-1, 2*hidden_units])
        self.y_reshaped = tf.reshape(self.y, [-1, num_classes])
        # check importantFunctions.py : line-40 to see how it works
        # reference link  is :
        # https://stackoverflow.com/questions/38051143/no-broadcasting-for-tf-matmul-in-tensorflow
        self.y_predicted = tf.nn.softmax(tf.matmul(self.outputs_reshaped, self.weights) + self.biases)

        # Define the loss function
        self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=self.y_predicted, labels=self.y_reshaped)

        # Define the trainer and optimizer
        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate)
        self.trainer = self.optimizer.minimize(self.loss)

        # creating session and initializing variables
        self.sess = tf.Session()
        self.init = tf.global_variables_initializer()
        self.sess.run(self.init)

        # get accuracy
        self.get_equal = tf.equal(tf.argmax(self.y_reshaped, 1), tf.argmax(self.y_predicted, 1))
        self.accuracy = tf.reduce_mean(tf.cast(self.get_equal, tf.float32))

    def predict(self, x, y):
        result = self.sess.run(self.y_predicted, feed_dict={self.x: x, self.y: y})
        return result

    def optimize(self, x, y):
        print("Accuracy on next-to-be-trained batch : ", self.sess.run(self.accuracy, feed_dict={self.x: x, self.y: y}))
        result = self.sess.run(self.trainer, feed_dict={self.x: x, self.y: y})
        print("Accuracy obtained after training     : ", self.sess.run(self.accuracy, feed_dict={self.x: x, self.y: y}))

    def cross_validate(self, x, y):
        result = self.sess.run(self.accuracy, feed_dict={self.x: x, self.y: y})
        return result

    def build_graph(self, x, y):
        writer = tf.summary.FileWriter('./graphs/lstmForPSSP',self.sess.graph)



model = BrnnForPssp(learning_rate=learning_rate, num_classes=8, hidden_units=hidden_units)
print("Successfully created the model")

for i in range(n_epochs):
    if i % 10 == 0:
        x = test_data_input
        y = test_data_otput
        print(i, "td - ", model.cross_validate(x=x, y=y))
    j = i%57
    x = train_data_input[j*100:j*100+100, :]
    y = train_data_otput[j*100:j*100+100, :]
    print("Iteration number & batch number: ", i, j)
    model.optimize(x=x, y=y)





